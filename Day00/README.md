# День 00 — Сбор данных

> **Перед стартом выполнения первого проекта, пожалуйста, пройди опрос по [ссылке](http://opros.so/ezyVA).**

## Проект по сбору данных
Этот проект позволит тебе погрузиться в процесс сбора данных, который может иметь множество подводных камней и отнимать немало времени.

## Оглавление

1. [Глава I](#глава-i) \
    1.1. [Преамбула](#преамбула)
2. [Глава II](#глава-ii) \
    2.1. [Общая инструкция](#общая-инструкция)
3. [Глава III](#глава-iii) \
    3.1. [Цели](#цели)
4. [Глава IV](#глава-iv) \
    4.1. [Задание](#задание)
5. [Глава V](#глава-v) \
    5.1. [Сдача работы и проверка](#сдача-работы-и-проверка)

## Глава I
### Преамбула

Человечество стало использовать данные довольно давно. Еще древние государства старались проводить перепись населения, чтобы примерно понимать, на какой объем налогов можно рассчитывать в следующий период. Или пастухи оценивали поголовье скота, чтобы знать, сколько они смогут заработать. То есть идея о том, что данные ведут к лучшим решениям, не нова. Данные позволяют снизить уровень неопределенности: намного проще ориентироваться в пространстве, где есть хотя бы тусклый свет, чем там, где кромешная тьма.

Изначально использование данных было простым и тривиальным: что-то посчитать, а потом из этого вывести другие показатели простыми арифметическими действиями. Но наука не стояла на месте, и в какой-то момент появились более сложные алгоритмы, которые дали возможность прогнозировать — то есть опять же переходить из состояния полной неопределенности в состояние меньшей неопределенности. Да, эти алгоритмы редко давали 100%-ую точность, но логика «всё или ничего» редко бывает продуктивной: если мы можем хоть немного снизить неопределенность, то это уже принесет свои плоды. Хотя, конечно, всё зависит от того, сколько мы должны затратить ресурсов и какие выгоды с этого получим. Простая экономическая целесообразность применима и здесь. И именно этот фактор сыграл важную роль в дальнейшем развитии этой области, и стал причиной того, почему мы сейчас видим этот бум на большие данные.

Человечество в какой-то момент уперлось в потолок точности моделей. Чтобы повышать точность дальше, требовалось больше данных и использование более сложных алгоритмов. Хранить такие объемы данных и обрабатывать их было слишком дорого. С точки зрения экономики, зачастую находиться в состоянии неопределенности было выгоднее, чем запускать проект по анализу данных. Многое поменялось после того, как стоимость «железа» с течением времени начала снижаться. То есть хранить и обрабатывать 1ГБ данных становилось всё дешевле и дешевле. Вместе с тем стали появляться и инструменты, которые позволили хранить и обрабатывать данные в распределенном режиме сразу на нескольких серверах, объединенных в кластеры, а не на суперкомпьютере, который стоил и продолжает стоить космических денег. Добавим сюда, что эти инструменты стали open-source, и это серьезным образом снизило барьер для входа в эту область — обработки больших данных. Потенциально эту технологию теперь может позволить в той или иной степени любая компания.

Но, конечно же, большие данные — это не серебряная пуля, которая способна решить все проблемы на планете. Для руководителя очень важно осознавать возможности и ограничения data science. Нужно обладать реалистичным пониманием, где есть основные проблемы и на что тратится больше всего времени в процессе анализа данных, а также какие существуют риски использования моделей.

Этот проект предлагает ровно это — прожить на себе путь анализа данных «от» и «до» и найти ответы на волнующие вопросы об этой области. Ниже ты можешь увидеть, как и чему будут посвящены проекты этой программы:
<details><summary>Расписание проектов</summary>

**День 00. Сбор данных** ← Ты находишься здесь: \
Данные находятся в разрозненных источниках. Их надо собрать, объединить в единый датасет и разобраться, какие данные у нас есть.

**День 01. Анализ данных:** \
Работа с простыми дескриптивными статистиками. Цель — чуть лучше понять анализ данных, выявить дополнительные проблемы с качеством данных и решить их. 
Ты построишь гистограммы, разные графики, чтобы еще лучше понять, как устроены данные. 
Всё это может дать идеи для создания новых продуктов.

**День 02. Машинное обучение:** \
Займемся задачей предсказания оттока клиентов. Подготовим данные к обучению.
Обучим модель машинного обучения. Оценим качество предсказания.

**День 03. Глубокое обучение:** \
Познакомимся с несколькими моделями глубокого обучения. 

**День 04. Внедрение:** \
Используя искусственный интеллект, ты разработаешь идею улучшения существующего процесса в своей сфере. Ты оценишь финансовый эффект от модели, трудозатраты, необходимые ресурсы, какой точности нужно добиться и с кем из стейкхолдеров следует переговорить.
</details>


## Глава II
### Общая инструкция

Методология «Школы 21» может быть не похожа на тот образовательный опыт, который случался с тобой ранее. Её отличает высокий уровень автономии: у тебя есть задача, и ты должен её выполнить. По большей части тебе нужно будет самому добывать знания для её решения. Второй важный момент — это peer-to-peer обучение. В образовательном процессе нет менторов и экспертов, перед которыми ты защищаешь свой результат. Ты это делаешь перед такими же учащимися, как и ты сам. У них есть чек-лист, который поможет им качественно выполнить приемку твоей работы.

Роль «Школы 21» заключается в том, чтобы обеспечить через последовательность заданий и оптимальный уровень поддержки такую траекторию обучения, при которой ты не только освоишь hard skills, но и научишься самообучаться.

* Не доверяй слухам и предположениям о том, как должно быть оформлено твое решение. Этот документ является единственным источником, к которому стоит обращаться по большинству вопросов.
* Твое решение будет оцениваться другими учащимися.
* Подлежат оцениванию только те файлы, которые ты сдал на проверку.
* Cдавай на проверку только те файлы, что были указаны в задании.
* Не забывай, что у тебя есть доступ к Интернету и поисковым системам.
* Обсуждение заданий можно вести и в мессенджерах.
* Будь внимателен к примерам, указанным в этом документе — они могут иметь важные детали, которые не были оговорены другим способом.
* И да пребудет с тобой Сила!


## Глава III
### Цель

Этот проект посвящен сбору данных. Мы не хотели давать тебе сразу готовый и правильно вычищенный датасет, потому что в реальной жизни такого не бывает, а этот проект старается воссоздать именно то, что происходит на самом деле в процессе анализа данных.

С одной стороны, это немного нудное занятие, но с другой — отличная разминка, мягкий вход в незнакомую сферу деятельности. Кроме того, для выполнения этого задания тебе придется пообщаться с другими учащимися, наладить взаимодействие, которое тебе не раз пригодится во время прохождения последующих проектов. Каждый студент «Школы 21» знает о пользе «роевого интеллекта».

## Глава IV
### Задание

Согласно исследованиям, дата-сайентисты тратят до 70-80% времени на сбор, подготовку данных и их понимание, нежели непосредственно на использование алгоритмов машинного обучения.

Сегодня ты начинаешь большой проект. Тебе, как дата-сайентисту, дали задачу — построить модель прогнозирования оттока клиентов. Сложно проводить анализ данных, когда у тебя их нет в полном объеме. Их нужно получить. У тебя есть только часть кусочков. Другие части датасета находятся у коллег в других департаментах. Твои части датасета будут присланы тебе индивидуально.

В этом задании ты начнешь знакомиться с Google Colab — основным инструментом работы дата-сайентиста. При помощи него тебе нужно будет решить поставленную задачу. Ноутбук этого проекта (src/day-00-assignment.ipynb), как и другие файлы к этому проекту, ты сможешь найти по [ссылке](https://disk.yandex.ru/d/ktyZk8PdLMoHyA). В ноутбуке есть подсказки — полезные команды.


## Задание 1
Изучи файл `datasets/attribute.xlsx` с атрибутами. Узнай, какие данные мы можем использовать для анализа.

В клетке ниже запиши, на **сколько предметных областей** разбиты атрибуты. Также предложи **несколько** атрибутов, которых у нас нет, но которые пригодились бы в анализе. Запиши их **списком**.

## Задание 2
Каждому из вас на почту прислали кусочки датасета за определенный период в определенной предметной области. Найди способ обменяться кусочками датасета со своими коллегами. Cохрани все найденные кусочки на компьютер в отдельную папку.

В клетке ниже **запиши, сколько файлов с данными у тебя получилось** (включая те, что были у тебя изначально).

## Задание 3
Загрузи все данные, которые ты собрал. 

Выведи количество строк в данных по любой предметной области за **второй квартал** 2021 года.

## Задание 4
Попарно объедини друг с другом данные по одинаковым предметным областям для каждой предметной области.

Выведи количество строк в данных по любой предметной области за **первое полугодие 2021 года**.

## Задание 5
Объедини таблицы из разных предметных областей в одну итоговую таблицу. В этом тебе пригодится функция [pd.merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html). 
После каждого объединения таблиц удаляй колонку `client_id` с помощью метода [.drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) \
Выведи, сколько **строк и столбцов** получилось в итоговой таблице.

## Глава V
### Сдача работы и проверка
1. Сохрани решения в файле day-00-assignment.ipynb. Затем скачай его из Google Colab. Для этого нажми на кнопку «Файлы» на панели меню --> «Загрузить как» --> Формат .ipynb
2. Загрузи файл в любое облачное хранилище (например, Яндекс Диск или Google Диск) и предоставь общий доступ читателя по ссылке. Затем скопируй ссылку.
3. Cоздай документ формата docx или pdf и вставь в него ссылку на твое решение.
4. Прикрепи документ в раздел «Решение» на платформе. 

